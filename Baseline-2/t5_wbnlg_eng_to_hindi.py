# -*- coding: utf-8 -*-
"""T5_WbNLG_Eng_to_Hindi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11rKQAOIIIc2LdFH9acUZHLNi7OX_meF6
"""

import glob
import os
import re
import xml.etree.ElementTree as ET
import pandas as pd
import time
import torch
from transformers import MT5Tokenizer, MT5ForConditionalGeneration,Adafactor

train_df=pd.read_csv("webNLG_Hindi_Final.csv", index_col=[0])
#train_df=train_df.iloc[  :10000,:]
train_df=train_df.sample(frac = 1)
batch_size=4
num_of_batches=len(train_df)/batch_size

if torch.cuda.is_available():
   dev = torch.device("cuda")
   print("Running on the GPU")
else:
   dev = torch.device("cpu")
   print("Running on the CPU")

#tokenizer = T5Tokenizer.from_pretrained("t5-base")
#model = T5ForConditionalGeneration.from_pretrained("t5-base", return_dict=True)

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")
model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small')
model = torch.nn.DataParallel(model)

#moving the model to GPU
model.to(dev)

optimizer = Adafactor(model.parameters(),lr=1e-3,
                      eps=(1e-30, 1e-3),
                      clip_threshold=1.0,
                      decay_rate=-0.8,
                      beta1=None,
                      weight_decay=0.0,
                      relative_step=False,
                      scale_parameter=False,
                      warmup_init=False)

import time

#Sets the module in training mode
model.train()
num_of_epochs = 10
num_of_batches = int(num_of_batches)
loss_per_10_steps=[]
for epoch in range(1,num_of_epochs+1):
  start = time.time()
  print('Running epoch: {}'.format(epoch))

  running_loss=0

  for i in range(num_of_batches):
    inputbatch=[]
    labelbatch=[]
    new_df=train_df[i*batch_size:i*batch_size+batch_size]
    for indx,row in new_df.iterrows():
      input = 'WebNLG: '+row['input_text']+'</s>'
      labels = row['target_text_hindi']+'</s>'
      inputbatch.append(input)
      labelbatch.append(labels)
    inputbatch=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')["input_ids"]
    labelbatch=tokenizer.batch_encode_plus(labelbatch,padding=True,max_length=400,return_tensors="pt") ["input_ids"]
    inputbatch=inputbatch.to(dev)
    labelbatch=labelbatch.to(dev)

    # clear out the gradients of all Variables
    optimizer.zero_grad()

    # Forward propogation
    outputs = model(input_ids=inputbatch, labels=labelbatch)
    loss = outputs.loss
    #loss_num=loss.item()
    loss_num=loss.mean()
    logits = outputs.logits
    running_loss+=loss_num
    if i%10 ==0:
      loss_per_10_steps.append(loss_num)

    # calculating the gradients
    loss.mean().backward()

    #updating the params
    optimizer.step()

  running_loss=running_loss/int(num_of_batches)
  print('Epoch: {} , Running loss: {} , time taken: {}'.format(epoch,running_loss,(time.time()-start)))

torch.save(model.state_dict(),'pytorch_hindi_model.bin')
#model.save_pretrained('./Saved/')

loaded = torch.load('pytorch_hindi_model.bin')
model = MT5ForConditionalGeneration.from_pretrained('google/mt5-small', state_dict = loaded, return_dict=True,config='t5-base-config.json')

tokenizer = MT5Tokenizer.from_pretrained("google/mt5-small")

def generate(text):
  model.eval()
  input_ids = tokenizer.encode("WebNLG:{} </s>".format(text), return_tensors="pt")  # Batch size 1
  #tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=400,return_tensors='pt')["input_ids"]
  print('input_ids:',input_ids)
  # input_ids.to(dev)
  s = time.time()
  outputs = model.generate(input_ids)
  print('outputs:',outputs)
  gen_text=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')
  elapsed = time.time() - s
  print('Generated by Plain Model in {} seconds'.format(str(elapsed)[:4]))


  return gen_text

#import time
#generate('area | hot | 11.35')
print(generate('Luciano_Spalletti | club | Udinese_Calcio && A.S._Roma | manager | Luciano_Spalletti') )
#generate('Bananaman | lastAired | "1986-04-15" && Bananaman | creator | Steve_Bright')



import torch.quantization
import torch.nn as nn

quantized_model = torch.quantization.quantize_dynamic(
    model, {nn.Linear,nn.Dropout,nn.LayerNorm}, dtype=torch.qint8
)

def quant_generate(text):
  quantized_model.eval()
  input_ids = tokenizer.encode("WebNLG:{} </s>".format(text), return_tensors="pt")  # Batch size 1
  # input_ids.to(dev)
  s = time.time()
  outputs = quantized_model.generate(input_ids)
  gen_text=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')
  elapsed = time.time() - s
  print('Generated by Quantized Model in {} seconds'.format(str(elapsed)[:4]))


  return gen_text

#import time
print(generate(' Russia | leader | Putin'))
print(quant_generate(' Russia | leader | Putin'))
print(generate('Sidhath | profession | Doctor  && Sidharth | home_town |  Bombay'))
print(quant_generate('Sidhath | profession | Doctor  && Sidharth | home_town |  Bombay'))
