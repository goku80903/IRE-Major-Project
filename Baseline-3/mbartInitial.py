# -*- coding: utf-8 -*-
"""mbart.ipynb.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1ALG4Rqq1muwMVB7IYtaLe64jGykEnT
"""

import pandas as pd
df_train = pd.read_csv('./input/df_train.csv')
df_validation = pd.read_csv('./input/df_validation.csv')
df_test = pd.read_csv('./input/df_test.csv')
# %% [code]


from torch.utils.data import Dataset, DataLoader
# df_train = pd.read_csv('df_train.csv')
# df_test = pd.read_csv('df_test.csv')
# df_validation = pd.read_csv('df_validation.csv')
def get_preprend_token(x): return 'russian' if 'Russian:' in x else 'hindi' if 'Hindi:' in x else 'english'

class DS(Dataset):
    def __init__(self, df):
        self.df = df.reset_index(drop=True)
        self.df['lang'] = self.df['input'].map(get_preprend_token)
        self.df = self.df[self.df.lang=='hindi']
    def __len__(self):
            return self.df.shape[0]
    def __getitem__(self, idx):
            row = self.df.iloc[idx]
            return row['input'],row['target'],row['lang']
train_ds = DS(df_train)
validation_ds = DS(df_validation)
test_ds = DS(df_test)
print(len(train_ds))
print(len(validation_ds))
print(len(test_ds))

train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)
validation_loader = DataLoader(validation_ds, batch_size=4, shuffle=False)
test_loader = DataLoader(test_ds, batch_size=4, shuffle=False)

from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, AutoTokenizer
import torch
from torch import optim

model = MBartForConditionalGeneration.from_pretrained("sshleifer/tiny-mbart")

#hindi_tokenizer = MBart50TokenizerFast.from_pretrained("sshleifer/tiny-mbart", src_lang="en_XX", tgt_lang="hi_IN")
#russian_tokenizer = MBart50TokenizerFast.from_pretrained("sshleifer/tiny-mbart", src_lang="en_XX", tgt_lang="ru_RU")
#english_tokenizer = MBart50TokenizerFast.from_pretrained("sshleifer/tiny-mbart", src_lang="en_XX", tgt_lang="en_XX")
hindi_tokenizer = AutoTokenizer.from_pretrained("sshleifer/tiny-mbart", src_lang="en_XX", tgt_lang="hi_IN")
#russian_tokenizer = AutoTokenizer.from_pretrained("sshleifer/tiny-mbart", src_lang="en_XX", tgt_lang="ru_RU")
english_tokenizer = AutoTokenizer.from_pretrained("sshleifer/tiny-mbart", src_lang="en_XX", tgt_lang="en_XX")

tokenizer = {'english':english_tokenizer,
#            'russian':russian_tokenizer,
            'hindi':hindi_tokenizer}

english_tokenizer.add_tokens(['<S>', '<R>','<O>'])

model.resize_token_embeddings(len(english_tokenizer))
#model = torch.nn.DataParallel(model)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)
optimizer = optim.AdamW(model.parameters(),lr =1e-3)

from tqdm import tqdm
import numpy as np

def validate(model,):
    model.eval()
    losses=[]
    for data in validation_loader:
        input_string,target_string,lang = data
        encoding = english_tokenizer(list(input_string),
                          padding='longest',
                          max_length=100,
                          truncation=True,
                          return_tensors="pt")
        input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

        max_length =max([len(i) for i in target_string ])

        target_encoding = []
        for l,t in zip(lang,target_string):
            with tokenizer[l].as_target_tokenizer():
                target_encoding.append([
                    label if label != tokenizer[l].pad_token_id else -100
                    for label in
                    tokenizer[l](t,padding='max_length',max_length =max_length, return_tensors="pt").input_ids[0]])


        labels = torch.tensor(target_encoding)

        # forward pass
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        labels = labels.to(device)
#         print(input_ids.shape)

        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
#         print(loss)
        losses.append(loss.tolist())
#         break
    print('Evaluation loss ',np.array(losses).mean())
    model.train()
# validate(model)

for epoch in range(10):
    losses=[]

    for batch,data in tqdm(enumerate(train_loader),total=df_train.shape[0]):
        input_string,target_string,lang = data
        optimizer.zero_grad()
        encoding = english_tokenizer(list(input_string),
                          padding='longest',
                          max_length=100,
                          truncation=True,
                          return_tensors="pt")
        input_ids, attention_mask = encoding.input_ids, encoding.attention_mask

        max_length =max([len(i) for i in target_string ])

        target_encoding = []
        for l,t in zip(lang,target_string):
            with tokenizer[l].as_target_tokenizer():
                target_encoding.append([
                    label if label != tokenizer[l].pad_token_id else -100
                    for label in
                    tokenizer[l](t,padding='max_length',max_length =max_length, return_tensors="pt").input_ids[0]])


        labels = torch.tensor(target_encoding)

        # forward pass
        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)
        labels = labels.to(device)
        loss = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss
        loss.backward()
        optimizer.step()
        losses.append(loss.tolist())
        #if batch%500==499:
        #    print('Training Loss ',np.array(losses).mean())

    print("Epoch:", epoch, " Loss: ", np.array(losses).mean())
    with torch.no_grad():
        validate(model,)
    #model.save_pretrained('./model')
    torch.save(model.state_dict(),'pytorch_mbart_model.bin')
    for language in ['english','hindi']:
        tokenizer[language].save_pretrained('./model_'+language)

from torchtext.data.metrics import bleu_score

lang_code = {'english':'en_XX','hindi':'hi_IN','russian':'ru_RU'}
def get_text(inp,lang):
    with torch.no_grad():

        otp_texts=[]
        for t,l in zip(inp,lang):
            test_input_ids = tokenizer['english']([ t],padding='longest',
                          max_length=100,
                          truncation=True,
                          return_tensors="pt")
            test_input_ids = test_input_ids.to(device)
            test_outputs = model.generate(**test_input_ids,forced_bos_token_id=tokenizer[l].lang_code_to_id[lang_code[l]])
            test_outputs = test_outputs.to(torch.device('cpu'))
            otp_texts.append(tokenizer[l].decode(test_outputs[0], skip_special_tokens=True))
        return otp_texts
def predict(loader):
    i_s,t_s,l_s,p_s = [],[],[],[]
    for batch in loader:
        i,t,l = batch
        i_s.extend(i)
        t_s.extend(t)
        l_s.extend(l)
        p_s.extend(get_text(list(i),l))
    return pd.DataFrame({'input':i_s,'target':t_s,'language':l_s,'prediction':p_s})

def print_score(d):
    y_true,y_pred = [],[]
    for text in tqdm(d['input'].unique()):
        y_true.append(d[d['input']==text]['target'].unique().tolist())
        y_pred.append(d[d['input']==text]['prediction'].unique().tolist())
    s = bleu_score([i[0].split() for i in y_pred], [[j.split() for j in i] for i in y_true])
    return s

d = predict(test_loader)
d.to_csv('predictions_on_test.csv',index=False)
print('Bleu Score on test set ',print_score(d))

d = predict(train_loader)
d.to_csv('predictions_on_train.csv',index=False)
print('Bleu Score on train set ',print_score(d))

d = predict(validation_loader)
d.to_csv('predictions_on_validation.csv',index=False)
print('Bleu Score on validation set ',print_score(d))
